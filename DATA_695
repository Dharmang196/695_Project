{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9092565,"sourceType":"datasetVersion","datasetId":5338818},{"sourceId":83541,"sourceType":"modelInstanceVersion","modelInstanceId":70174,"modelId":95269},{"sourceId":83544,"sourceType":"modelInstanceVersion","modelInstanceId":70176,"modelId":95271},{"sourceId":84138,"sourceType":"modelInstanceVersion","modelInstanceId":70671,"modelId":95715},{"sourceId":84140,"sourceType":"modelInstanceVersion","modelInstanceId":70673,"modelId":95717}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Appendix Stage 1","metadata":{}},{"cell_type":"markdown","source":"# Step 1","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef generate_health_data(n_records, seed, dataset_id, alter_relationships=False):\n    np.random.seed(seed + dataset_id)  # Ensure different seeds for different datasets\n    age = np.random.randint(10, 65, size=n_records)\n    meds = ['drug1', 'drug2', 'drug3', 'drug4']\n    postal_codes = [f'PC-{i:02d}' for i in range(1, 31)]\n\n    # Initialize data dictionary\n    data = {\n        'Patient ID': ['PID-' + str(i) for i in range(n_records)],\n        'Age': age,\n        'Sex': np.random.choice(['Male', 'Female'], n_records),\n        'ADHD Medication': [],\n        'Time on Medication': [],\n        'Switched Medication': [],\n        'Postal Code': np.random.choice(postal_codes, n_records)\n    }\n\n    # One-hot encoded columns for non-ADHD drugs\n    for i in range(1, 11):\n        data[f'NonADHD_Drug_{i}'] = np.random.choice([0, 1], size=n_records)\n\n    for i in range(n_records):\n        # Adjust medication probabilities based on age and postal code\n        if age[i] < 25:\n            prob_drugs = [0.4, 0.4, 0.1, 0.1] if not alter_relationships else [0.1, 0.1, 0.4, 0.4]\n        else:\n            prob_drugs = [0.1, 0.1, 0.4, 0.4] if not alter_relationships else [0.4, 0.4, 0.1, 0.1]\n\n        if data['Postal Code'][i] in ['PC-05', 'PC-15', 'PC-10', 'PC-20']:\n            prob_drugs = [0.2, 0.2, 0.3, 0.3] if not alter_relationships else [0.3, 0.3, 0.2, 0.2]\n\n        chosen_med = np.random.choice(meds, p=prob_drugs)\n        data['ADHD Medication'].append(chosen_med)\n\n        # Calculate base time on medication influenced by age, non-ADHD drugs, and sex\n        base_time = 50 * np.log(age[i] / 10 + 1) + np.random.normal(50, 10)\n        non_adhd_influence = sum(data[f'NonADHD_Drug_{j}'][i] for j in range(1, 11)) * 2\n        base_time += non_adhd_influence\n\n        # Complex Non-Linear Relationship: Polynomial, Trigonometric, Exponential, and Interaction functions\n        if data['Sex'][i] == 'Male':\n            poly_term = 0.1 * (age[i] ** 2) - 0.05 * (age[i] ** 3 / 1000) + 0.01 * (age[i] ** 4 / 10000)\n            trig_term = 20 * np.sin(age[i] / 10) + 10 * np.cos(age[i] / 15) + 5 * np.sin(age[i] / 5)\n            exp_term = 10 * np.exp(age[i] / 50)\n            interaction_term = 5 * np.log(age[i] + 1) * np.sin(age[i] / 10)\n            base_time += poly_term + trig_term + exp_term + interaction_term\n        else:\n            poly_term = 0.15 * (age[i] ** 2) - 0.07 * (age[i] ** 3 / 1000) + 0.02 * (age[i] ** 4 / 10000)\n            trig_term = 25 * np.sin(age[i] / 7) + 12 * np.cos(age[i] / 10) + 6 * np.sin(age[i] / 4)\n            exp_term = 30 * np.exp(age[i] / 50) + 5 * np.exp(age[i] / 100)\n            interaction_term = 8 * np.log(age[i] + 1) * np.cos(age[i] / 8)\n            base_time += poly_term + trig_term + exp_term + interaction_term\n\n        # Adjust base time if patient is on specific drugs that may react with sex\n        if data['Sex'][i] == 'Female' and chosen_med in ['drug3', 'drug4']:\n            base_time *= 1.05  # Assuming these drugs have different impacts based on sex\n\n        # Complex Linear Relationship: Interaction between age, sex, and number of non-ADHD drugs\n        non_adhd_count = sum(data[f'NonADHD_Drug_{j}'][i] for j in range(1, 11))\n        if data['Sex'][i] == 'Male':\n            base_time += (0.5 * age[i]) + (0.1 * age[i] * non_adhd_count)\n        else:\n            base_time -= (0.3 * age[i]) + (0.05 * age[i] * non_adhd_count)\n\n        if chosen_med in ['drug3', 'drug4']:\n            base_time *= 1.1 if not alter_relationships else 0.9\n\n        if data['Postal Code'][i] in ['PC-05', 'PC-15', 'PC-10', 'PC-20']:\n            base_time += 15 if not alter_relationships else -15\n        \n        data['Time on Medication'].append(base_time)\n\n        # Complex Non-Linear Relationship: Adjust switch probability with more complex functions\n        switch_prob = 0.1 * np.tanh(0.05 * (age[i] - 40)) + 0.05 * (base_time / 200) + 0.01 * np.sin(age[i]) + 0.02 * np.cos(age[i] / 2)\n        switch_prob += 0.1 * np.exp(age[i] / 60)  # Additional complexity with exponential term\n        data['Switched Medication'].append('Yes' if np.random.rand() < switch_prob else 'No')\n\n    return pd.DataFrame(data)\n\n# Example usage\nsizes = [500, 1000, 5000, 10000, 20000]\nnum_copies = 5  # Number of datasets to generate for each size\nbase_path = '/kaggle/working/'  # Path where the datasets are stored\n\nfor size in sizes:\n    for copy in range(num_copies):\n        df = generate_health_data(size, seed=42, dataset_id=copy)  # Vary seed to generate different data\n        filename = f'generated_data_{size}_{copy}.csv'\n        df.to_csv(filename, index=False)\n        print(f'Dataset with {size} records, copy {copy + 1}, saved as {filename}')\n\n# Visualizing example dataset\ndf = generate_health_data(1000, seed=42, dataset_id=0)\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='Age', y='Time on Medication', hue='ADHD Medication', style='Switched Medication', data=df)\nplt.title('Relationship between Age, Time on Medication, ADHD Medication, and Switching Status')\nplt.xlabel('Age')\nplt.ylabel('Time on Medication')\nplt.legend(title='Medication / Switched')\nplt.show()\n\n# Additional visualizations\n# Visualizing the influence of postal code on medication type\nplt.figure(figsize=(12, 7))\nsns.countplot(x='Postal Code', hue='ADHD Medication', data=df)\nplt.title('Distribution of Medication Types by Postal Code')\nplt.xlabel('Postal Code')\nplt.ylabel('Count')\nplt.legend(title='Medication Type')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Analyzing the relationship between sex and medication type\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Sex', hue='ADHD Medication', data=df)\nplt.title('Medication Type by Sex')\nplt.xlabel('Sex')\nplt.ylabel('Count')\nplt.legend(title='Medication Type')\nplt.show()\n\n# Examining the effect of the total count of Non-ADHD Drugs on the likelihood of switching medications\ndf['Total Non-ADHD Drugs'] = df[[f'NonADHD_Drug_{i}' for i in range(1, 11)]].sum(axis=1)\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='Switched Medication', y='Total Non-ADHD Drugs', data=df)\nplt.title('Effect of Non-ADHD Drug Count on Medication Switching')\nplt.xlabel('Switched Medication')\nplt.ylabel('Total Non-ADHD Drugs')\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-17T21:07:50.728749Z","iopub.execute_input":"2024-07-17T21:07:50.729121Z","iopub.status.idle":"2024-07-17T21:08:18.130093Z","shell.execute_reply.started":"2024-07-17T21:07:50.729091Z","shell.execute_reply":"2024-07-17T21:08:18.128957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'df' is your DataFrame and you want to check the relationship between 'Age' and 'Time on Medication'\nsns.scatterplot(x='Age', y='Time on Medication', hue='Sex', data=df)\nplt.title('Scatter Plot of Age vs. Time on Medication by Sex')\nplt.show()\n\nimport statsmodels.api as sm\n\n# Fit a regression model\nX = sm.add_constant(df['Age'])  # adding a constant for the intercept\nmodel = sm.OLS(df['Time on Medication'], X).fit()\n\n# Plotting the residuals\ndf['Predictions'] = model.predict(X)\ndf['Residuals'] = df['Time on Medication'] - df['Predictions']\nsns.scatterplot(x='Predictions', y='Residuals', data=df)\nplt.title('Residual Plot')\nplt.axhline(0, color='red', linestyle='--')  # a horizontal line at zero for reference\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T21:08:21.986407Z","iopub.execute_input":"2024-07-17T21:08:21.987176Z","iopub.status.idle":"2024-07-17T21:08:24.391113Z","shell.execute_reply.started":"2024-07-17T21:08:21.987138Z","shell.execute_reply":"2024-07-17T21:08:24.389970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\ndef preprocess_and_save_data(filepath, output_filepath):\n    # Load the dataset\n    df = pd.read_csv(filepath)\n    \n    # Split the dataset into training and testing sets\n    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n    # Initialize LabelEncoders and save them for later use\n    le_sex = LabelEncoder()\n    le_adhd_med = LabelEncoder()\n    le_postal_code = LabelEncoder()\n\n    # Fit LabelEncoders on training data and transform both training and testing sets\n    train_df['Sex'] = le_sex.fit_transform(train_df['Sex'])\n    test_df['Sex'] = le_sex.transform(test_df['Sex'])\n    train_df['ADHD Medication'] = le_adhd_med.fit_transform(train_df['ADHD Medication'])\n    test_df['ADHD Medication'] = le_adhd_med.transform(test_df['ADHD Medication'])\n    train_df['Postal Code'] = le_postal_code.fit_transform(train_df['Postal Code'])\n    test_df['Postal Code'] = le_postal_code.transform(test_df['Postal Code'])\n\n    # Initialize and fit the StandardScaler on training data\n    scaler = StandardScaler()\n    numeric_cols = ['Age', 'Time on Medication'] + [f'NonADHD_Drug_{i}' for i in range(1, 11)]\n\n    # Scale numeric features for both training and testing sets\n    train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])\n    test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n\n    # Concatenate back into a single DataFrame\n    df_preprocessed = pd.concat([train_df, test_df])\n\n    # Save the preprocessed dataframe and encoders/scalers to new files\n    df_preprocessed.to_csv(output_filepath, index=False)\n    with open('label_encoders.pkl', 'wb') as f:\n        pickle.dump([le_sex, le_adhd_med, le_postal_code], f)\n    with open('scaler.pkl', 'wb') as f:\n        pickle.dump(scaler, f)\n\n    return df_preprocessed\n\n# Example usage for each dataset size\nsizes = [500, 1000, 5000, 10000, 20000]\nnum_copies = 5  # Number of datasets to generate for each size\nbase_path = '/kaggle/working/'  # Path where the datasets are stored\n\nfor size in sizes:\n    for copy in range(num_copies):\n        input_path = f'{base_path}generated_data_{size}_{copy}.csv'\n        output_path = f'{base_path}preprocessed_data_{size}_{copy}.csv'\n        df_preprocessed = preprocess_and_save_data(input_path, output_path)\n        print(f'Preprocessed dataset with {size} records, copy {copy + 1}, saved as {output_path}')\n        if size == 1000 and copy == 0:\n            print(df_preprocessed.head())  # Optionally display the head of the DataFrame for the first copy of 1000 records\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T21:08:33.253837Z","iopub.execute_input":"2024-07-17T21:08:33.254491Z","iopub.status.idle":"2024-07-17T21:08:39.872209Z","shell.execute_reply.started":"2024-07-17T21:08:33.254454Z","shell.execute_reply":"2024-07-17T21:08:39.871023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3","metadata":{}},{"cell_type":"code","source":"\nfrom tensorflow.keras.models import Model, save_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import Input, Dense, Concatenate, Embedding, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport os\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\ndef create_complex_model(feature_columns, postal_code_vocab_size=30, l2_reg=0.04):\n    inputs = {name: Input(shape=(1,), name=name) for name in feature_columns}\n    embeddings = []\n    for name in feature_columns:\n        if name == 'Postal Code':\n            embedding = Embedding(input_dim=postal_code_vocab_size, output_dim=20)(inputs[name])\n            embedding = Flatten()(embedding)\n            embeddings.append(embedding)\n        else:\n            embeddings.append(inputs[name])\n\n    concatenated_inputs = Concatenate()(embeddings)\n    \n    # Add Dense layers with increased complexity\n    x = Dense(1024, activation='relu', kernel_regularizer=l2(l2_reg))(concatenated_inputs)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(512, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(256, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    # Additional Dense Layers\n    x = Dense(32, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(16, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(8, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    output = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=list(inputs.values()), outputs=output)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-07-31T17:09:21.288973Z","iopub.execute_input":"2024-07-31T17:09:21.289556Z","iopub.status.idle":"2024-07-31T17:09:34.555602Z","shell.execute_reply.started":"2024-07-31T17:09:21.289524Z","shell.execute_reply":"2024-07-31T17:09:34.554485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_local_models_with_repeats(paths, model_creator, feature_columns, postal_code_vocab_size, target_column='Switched Medication', epochs_map=None, base_batch_size=16, num_repeats=5):\n    models_path = '/kaggle/working/models/local/'\n    if not os.path.exists(models_path):\n        os.makedirs(models_path)\n    scaler = StandardScaler()\n\n    local_accuracies = {size: [] for size in paths.keys()}\n    \n    for size, path in paths.items():\n        for repeat in range(num_repeats):\n            df = pd.read_csv(path)\n            df = df.dropna(subset=feature_columns + [target_column])\n            train_df, test_df = train_test_split(df, test_size=0.2, random_state=42 + repeat)\n            \n            train_features = scaler.fit_transform(train_df[feature_columns])\n            train_features_dict = {name: train_features[:, j].reshape(-1, 1) for j, name in enumerate(feature_columns)}\n            train_labels = train_df[target_column].apply(lambda x: 1 if x == 'Yes' else 0).values\n\n            # Re-initialize the model for each repeat\n            model = model_creator(feature_columns, postal_code_vocab_size)\n            model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\n            early_stopping = EarlyStopping(monitor='val_loss', patience=60, restore_best_weights=True)\n            model.fit(train_features_dict, train_labels, epochs=epochs_map[size], batch_size=base_batch_size, validation_split=0.1, callbacks=[early_stopping])\n\n            test_features = scaler.transform(test_df[feature_columns])\n            test_features_dict = {name: test_features[:, j].reshape(-1, 1) for j, name in enumerate(feature_columns)}\n            test_labels = test_df[target_column].apply(lambda x: 1 if x == 'Yes' else 0).values\n            loss, accuracy = model.evaluate(test_features_dict, test_labels, verbose=0)\n            local_accuracies[size].append(accuracy)\n\n            print(f\"Repeat {repeat+1} - Local Model Accuracy for size {size}: {accuracy}\")\n\n        # Save the model once after all repeats for this dataset size\n        model_save_path = os.path.join(models_path, f'local_model_{size}.h5')\n        model.save(model_save_path)\n        print(f\"Local Model for size {size} saved at {model_save_path}\")\n\n    return local_accuracies\n\n# Training local models\nnum_repeats = 5\nlocal_paths = {size: f'/kaggle/input/inputdata18/preprocessed_data_{size}_0.csv' for size in [500, 1000, 5000, 10000]}\nepochs_map = {500: 55, 1000: 65, 5000: 75, 10000: 85}\n\nlocal_accuracies = train_local_models_with_repeats(local_paths, create_complex_model, ['Age', 'Sex', 'ADHD Medication', 'Postal Code', 'Time on Medication'], postal_code_vocab_size=30, epochs_map=epochs_map)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-27T15:17:46.105561Z","iopub.execute_input":"2024-07-27T15:17:46.106242Z","iopub.status.idle":"2024-07-27T15:50:26.496042Z","shell.execute_reply.started":"2024-07-27T15:17:46.106207Z","shell.execute_reply":"2024-07-27T15:50:26.495125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef train_federated_models(paths, model_creator, feature_columns, postal_code_vocab_size, target_column='Switched Medication', epochs_map=None, base_batch_size=16, learning_rate=0.0001):\n    models_path = '/kaggle/working/models/federated/'\n    if not os.path.exists(models_path):\n        os.makedirs(models_path)\n    scaler = StandardScaler()\n\n    federated_accuracies = {}\n\n    model = model_creator(feature_columns, postal_code_vocab_size)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n\n    for size, path_list in paths.items():\n        size_accuracies = []\n        for i, path in enumerate(path_list):\n            df = pd.read_csv(path)\n            df = df.dropna(subset=feature_columns + [target_column])\n            features = scaler.fit_transform(df[feature_columns])\n            features_dict = {name: features[:, j].reshape(-1, 1) for j, name in enumerate(feature_columns)}\n            labels = df[target_column].apply(lambda x: 1 if x == 'Yes' else 0).values\n\n            early_stopping = EarlyStopping(monitor='val_loss', patience=60, restore_best_weights=True)\n            model.fit(features_dict, labels, epochs=epochs_map[size], batch_size=base_batch_size, validation_split=0.1, callbacks=[early_stopping])\n\n            model_save_path = os.path.join(models_path, f'federated_model_{size}_{i}.h5')\n            model.save(model_save_path)\n            print(f'Model trained on dataset {size}_{i} saved as {model_save_path}')\n\n            # Evaluate and store accuracy for this model\n            loss, accuracy = model.evaluate(features_dict, labels, verbose=0)\n            size_accuracies.append({'loss': loss, 'accuracy': accuracy})\n        \n        federated_accuracies[size] = size_accuracies\n    \n    return federated_accuracies\n\n\n# Paths setup for training data\nbase_path = '/kaggle/input/inputdata18/'\nsizes = [500, 1000, 5000, 10000]\nfederated_paths = {size: [f'{base_path}preprocessed_data_{size}_{copy}.csv' for copy in range(5)] for size in sizes}\n\n# Define epochs for each size\nepochs_map = {500: 55, 1000: 65, 5000: 75, 10000: 85}\n\n# Train federated models and collect accuracies\nfederated_accuracies = train_federated_models(federated_paths, create_complex_model, ['Age', 'Sex', 'ADHD Medication', 'Postal Code', 'Time on Medication'], postal_code_vocab_size=30, epochs_map=epochs_map)\n\n# Print the accuracies for each federated model\nfor size, accuracies in federated_accuracies.items():\n    print(f\"Federated Model Accuracies for size {size}: {[acc['accuracy'] for acc in accuracies]}\")\n    print(f\"Federated Model Mean Accuracy for size {size}: {np.mean([acc['accuracy'] for acc in accuracies])}\")\n    print(f\"Federated Model Std Deviation for size {size}: {np.std([acc['accuracy'] for acc in accuracies])}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T17:09:50.386849Z","iopub.execute_input":"2024-07-31T17:09:50.387530Z","iopub.status.idle":"2024-07-31T17:40:08.588190Z","shell.execute_reply.started":"2024-07-31T17:09:50.387493Z","shell.execute_reply":"2024-07-31T17:40:08.587282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4","metadata":{}},{"cell_type":"code","source":"\nfrom tensorflow.keras.models import load_model\nimport os\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef evaluate_models_extended(local_models_path, federated_models_path, feature_columns, test_data_paths):\n    results = {'local': {}, 'federated': {}}\n    scaler = StandardScaler()\n\n    def evaluate(model, test_features, test_labels):\n        predictions = model.predict(test_features)\n        predicted_labels = (predictions > 0.5).astype(int)\n        accuracy = np.mean(predicted_labels == test_labels)\n        precision = precision_score(test_labels, predicted_labels)\n        recall = recall_score(test_labels, predicted_labels)\n        f1 = f1_score(test_labels, predicted_labels)\n        return accuracy, precision, recall, f1\n\n    # Evaluate federated models\n    federated_results = {}\n    for size, paths in test_data_paths.items():\n        size_results = []\n        for i, path in enumerate(paths):\n            model_path = os.path.join(federated_models_path, f'federated_model_{size}_{i}.h5')\n            federated_model = load_model(model_path)\n\n            df_test = pd.read_csv(path)\n            test_features_raw = scaler.fit_transform(df_test[feature_columns])\n            test_features = {name: test_features_raw[:, j].reshape(-1, 1) for j, name in enumerate(feature_columns)}\n            test_labels = df_test['Switched Medication'].apply(lambda x: 1 if x == 'Yes' else 0).values\n            \n            accuracy, precision, recall, f1 = evaluate(federated_model, test_features, test_labels)\n            size_results.append({'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1})\n        federated_results[size] = size_results\n    \n    results['federated'] = federated_results\n\n    # Evaluate local models\n    local_results = {}\n    for size, path_list in test_data_paths.items():\n        local_model_path = os.path.join(local_models_path, f'local_model_{size}.h5')\n        local_model = load_model(local_model_path)\n\n        df_test = pd.read_csv(path_list[0])\n        test_features_raw = scaler.fit_transform(df_test[feature_columns])\n        test_features = {name: test_features_raw[:, j].reshape(-1, 1) for j, name in enumerate(feature_columns)}\n        test_labels = df_test['Switched Medication'].apply(lambda x: 1 if x == 'Yes' else 0).values\n\n        accuracy, precision, recall, f1 = evaluate(local_model, test_features, test_labels)\n        local_results[size] = [{'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}]\n    \n    results['local'] = local_results\n\n    return results\n\n# Paths for the test datasets\ntest_data_paths = {size: [f'/kaggle/input/inputdata18/preprocessed_data_{size}_{copy}.csv' for copy in range(5)] for size in [500, 1000, 5000, 10000]}\n\nlocal_models_path = '/kaggle/working/models/local/'\nfederated_models_path = '/kaggle/working/models/federated/'\n\nevaluation_results = evaluate_models_extended(local_models_path, federated_models_path, ['Age', 'Sex', 'ADHD Medication', 'Postal Code', 'Time on Medication'], test_data_paths)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T17:42:40.959452Z","iopub.execute_input":"2024-07-31T17:42:40.960067Z","iopub.status.idle":"2024-07-31T17:43:22.883342Z","shell.execute_reply.started":"2024-07-31T17:42:40.960034Z","shell.execute_reply":"2024-07-31T17:43:22.882550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_final_accuracy_with_repeats(local_accuracies, federated_metrics, sizes, save_path):\n    plt.figure(figsize=(12, 8))\n\n    # Plot individual accuracies for local models\n    for size in sizes:\n        repeats = local_accuracies[size]\n        plt.scatter([size]*len(repeats), repeats, color='blue', alpha=0.5, label=f'Local Models' if size == sizes[0] else \"\")\n\n    # Plot individual accuracies for federated models\n    for size in sizes:\n        federated_repeats = [m['accuracy'] for m in federated_metrics[size]]\n        plt.scatter([size]*len(federated_repeats), federated_repeats, color='orange', alpha=0.5, label=f'Federated Models' if size == sizes[0] else \"\")\n\n    # Calculate means and std deviations for line chart with error bars\n    local_means = [np.mean(local_accuracies[size]) for size in sizes]\n    local_stds = [np.std(local_accuracies[size]) for size in sizes]\n\n    federated_means = [np.mean([m['accuracy'] for m in federated_metrics[size]]) for size in sizes]\n    federated_stds = [np.std([m['accuracy'] for m in federated_metrics[size]]) for size in sizes]\n\n    plt.errorbar(sizes, local_means, yerr=local_stds, marker='o', linestyle='-', color='blue')\n    plt.errorbar(sizes, federated_means, yerr=federated_stds, marker='o', linestyle='--', color='orange')\n\n    plt.xlabel('Dataset Size')\n    plt.ylabel('Accuracy')\n    plt.title('Accuracy vs. Dataset Size (Local and Federated Models)')\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(f'{save_path}/accuracy_comparison_repeats.png')\n    plt.show()\n\n# Plotting with individual accuracies of all repeats\nplot_final_accuracy_with_repeats(local_accuracies, evaluation_results['federated'], [500, 1000, 5000, 10000], '/kaggle/working/')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T17:44:21.878426Z","iopub.execute_input":"2024-07-31T17:44:21.878799Z","iopub.status.idle":"2024-07-31T17:44:22.329593Z","shell.execute_reply.started":"2024-07-31T17:44:21.878771Z","shell.execute_reply":"2024-07-31T17:44:22.328313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Appendix Stage 2","metadata":{}},{"cell_type":"markdown","source":"# Step 1","metadata":{}},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-08-03T01:50:11.265426Z","iopub.execute_input":"2024-08-03T01:50:11.266253Z","iopub.status.idle":"2024-08-03T01:50:12.631770Z","shell.execute_reply.started":"2024-08-03T01:50:11.266207Z","shell.execute_reply":"2024-08-03T01:50:12.630224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndef generate_health_data(n_records, seed, dataset_id, relationship_type):\n    np.random.seed(seed + dataset_id)  # Ensure different seeds for different datasets\n    age = np.random.randint(10, 65, size=n_records)\n    meds = ['drug1', 'drug2', 'drug3', 'drug4']\n    postal_codes = [f'PC-{i:02d}' for i in range(1, 31)]\n\n    data = {\n        'Patient ID': ['PID-' + str(i) for i in range(n_records)],\n        'Age': age,\n        'Sex': np.random.choice(['Male', 'Female'], n_records),\n        'ADHD Medication': [],\n        'Time on Medication': [],\n        'Switched Medication': [],\n        'Postal Code': np.random.choice(postal_codes, n_records)\n    }\n\n    for i in range(1, 11):\n        data[f'NonADHD_Drug_{i}'] = np.random.choice([0, 1], size=n_records)\n\n    for i in range(n_records):\n        if age[i] < 25:\n            if relationship_type == 1:\n                prob_drugs = [0.4, 0.4, 0.1, 0.1]\n            elif relationship_type == 2:\n                prob_drugs = [0.1, 0.1, 0.4, 0.4]\n            elif relationship_type == 3:\n                prob_drugs = [0.3, 0.3, 0.2, 0.2]\n            else:\n                prob_drugs = [0.25, 0.25, 0.25, 0.25]\n        else:\n            if relationship_type == 1:\n                prob_drugs = [0.1, 0.1, 0.4, 0.4]\n            elif relationship_type == 2:\n                prob_drugs = [0.4, 0.4, 0.1, 0.1]\n            elif relationship_type == 3:\n                prob_drugs = [0.2, 0.2, 0.3, 0.3]\n            else:\n                prob_drugs = [0.25, 0.25, 0.25, 0.25]\n\n        if data['Postal Code'][i] in ['PC-05', 'PC-15', 'PC-10', 'PC-20']:\n            if relationship_type == 1:\n                prob_drugs = [0.2, 0.2, 0.3, 0.3]\n            elif relationship_type == 2:\n                prob_drugs = [0.3, 0.3, 0.2, 0.2]\n            elif relationship_type == 3:\n                prob_drugs = [0.25, 0.25, 0.25, 0.25]\n            else:\n                prob_drugs = [0.35, 0.35, 0.15, 0.15]\n\n        chosen_med = np.random.choice(meds, p=prob_drugs)\n        data['ADHD Medication'].append(chosen_med)\n\n        base_time = 50 * np.log(age[i] / 10 + 1) + np.random.normal(50, 10)\n        non_adhd_influence = sum(data[f'NonADHD_Drug_{j}'][i] for j in range(1, 11)) * 2\n        base_time += non_adhd_influence\n\n        if data['Sex'][i] == 'Male':\n            poly_term = 0.1 * (age[i] ** 2) - 0.05 * (age[i] ** 3 / 1000) + 0.01 * (age[i] ** 4 / 10000)\n            trig_term = 20 * np.sin(age[i] / 10) + 10 * np.cos(age[i] / 15) + 5 * np.sin(age[i] / 5)\n            exp_term = 10 * np.exp(age[i] / 50)\n            interaction_term = 5 * np.log(age[i] + 1) * np.sin(age[i] / 10)\n            base_time += poly_term + trig_term + exp_term + interaction_term\n        else:\n            poly_term = 0.15 * (age[i] ** 2) - 0.07 * (age[i] ** 3 / 1000) + 0.02 * (age[i] ** 4 / 10000)\n            trig_term = 25 * np.sin(age[i] / 7) + 12 * np.cos(age[i] / 10) + 6 * np.sin(age[i] / 4)\n            exp_term = 30 * np.exp(age[i] / 50) + 5 * np.exp(age[i] / 100)\n            interaction_term = 8 * np.log(age[i] + 1) * np.cos(age[i] / 8)\n            base_time += poly_term + trig_term + exp_term + interaction_term\n\n        if data['Sex'][i] == 'Female' and chosen_med in ['drug3', 'drug4']:\n            base_time *= 1.05\n\n        non_adhd_count = sum(data[f'NonADHD_Drug_{j}'][i] for j in range(1, 11))\n        if data['Sex'][i] == 'Male':\n            base_time += (0.5 * age[i]) + (0.1 * age[i] * non_adhd_count)\n        else:\n            base_time -= (0.3 * age[i]) + (0.05 * age[i] * non_adhd_count)\n\n        if chosen_med in ['drug3', 'drug4']:\n            base_time *= 1.1 if relationship_type in [1, 3] else 0.9\n\n        if data['Postal Code'][i] in ['PC-05', 'PC-15', 'PC-10', 'PC-20']:\n            base_time += 15 if relationship_type in [1, 2] else -15\n\n        data['Time on Medication'].append(base_time)\n\n        switch_prob = 0.1 * np.tanh(0.05 * (age[i] - 40)) + 0.05 * (base_time / 200) + 0.01 * np.sin(age[i]) + 0.02 * np.cos(age[i] / 2)\n        switch_prob += 0.1 * np.exp(age[i] / 60)\n        data['Switched Medication'].append('Yes' if np.random.rand() < switch_prob else 'No')\n\n    return pd.DataFrame(data)\n\n# Example usage\nsizes = [500, 1000, 5000, 10000, 20000]\nnum_copies = 5  # Number of sub-datasets for each main dataset size\nbase_path = '/kaggle/working/'  # Path where the datasets are stored\n\n# Ensure unique causal relationships for each dataset size\nfor size_index, size in enumerate(sizes):\n    for copy in range(num_copies):\n        relationship_type = (size_index % 4) + 1  # Ensuring different relationships for each dataset size\n        df = generate_health_data(size, seed=42, dataset_id=size_index, relationship_type=relationship_type)\n        filename = f'{base_path}generated_data_{size}_{copy}_different_causal.csv'\n        df.to_csv(filename, index=False)\n\n# Display the relationship settings for each dataset\nprint(\"Datasets generated with different causal relationships for each size.\")\n","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-02T23:28:54.676803Z","iopub.execute_input":"2024-08-02T23:28:54.677208Z","iopub.status.idle":"2024-08-02T23:29:14.447383Z","shell.execute_reply.started":"2024-08-02T23:28:54.677172Z","shell.execute_reply":"2024-08-02T23:29:14.446361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'df' is your DataFrame and you want to check the relationship between 'Age' and 'Time on Medication'\nsns.scatterplot(x='Age', y='Time on Medication', hue='Sex', data=df)\nplt.title('Scatter Plot of Age vs. Time on Medication by Sex')\nplt.show()\n\nimport statsmodels.api as sm\n\n# Fit a regression model\nX = sm.add_constant(df['Age'])  # adding a constant for the intercept\nmodel = sm.OLS(df['Time on Medication'], X).fit()\n\n# Plotting the residuals\ndf['Predictions'] = model.predict(X)\ndf['Residuals'] = df['Time on Medication'] - df['Predictions']\nsns.scatterplot(x='Predictions', y='Residuals', data=df)\nplt.title('Residual Plot')\nplt.axhline(0, color='red', linestyle='--')  # a horizontal line at zero for reference\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-02T23:29:26.873368Z","iopub.execute_input":"2024-08-02T23:29:26.873728Z","iopub.status.idle":"2024-08-02T23:29:28.380831Z","shell.execute_reply.started":"2024-08-02T23:29:26.873699Z","shell.execute_reply":"2024-08-02T23:29:28.379902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\ndef preprocess_and_save_data(filepath, output_filepath):\n    # Load the dataset\n    df = pd.read_csv(filepath)\n    \n    # Split the dataset into training and testing sets\n    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n    # Initialize LabelEncoders and save them for later use\n    le_sex = LabelEncoder()\n    le_adhd_med = LabelEncoder()\n    le_postal_code = LabelEncoder()\n\n    # Fit LabelEncoders on training data and transform both training and testing sets\n    train_df['Sex'] = le_sex.fit_transform(train_df['Sex'])\n    test_df['Sex'] = le_sex.transform(test_df['Sex'])\n    train_df['ADHD Medication'] = le_adhd_med.fit_transform(train_df['ADHD Medication'])\n    test_df['ADHD Medication'] = le_adhd_med.transform(test_df['ADHD Medication'])\n    train_df['Postal Code'] = le_postal_code.fit_transform(train_df['Postal Code'])\n    test_df['Postal Code'] = le_postal_code.transform(test_df['Postal Code'])\n\n    # Initialize and fit the StandardScaler on training data\n    scaler = StandardScaler()\n    numeric_cols = ['Age', 'Time on Medication'] + [f'NonADHD_Drug_{i}' for i in range(1, 11)]\n\n    # Scale numeric features for both training and testing sets\n    train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])\n    test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n\n    # Concatenate back into a single DataFrame\n    df_preprocessed = pd.concat([train_df, test_df])\n\n    # Save the preprocessed dataframe and encoders/scalers to new files\n    df_preprocessed.to_csv(output_filepath, index=False)\n    with open('label_encoders.pkl', 'wb') as f:\n        pickle.dump([le_sex, le_adhd_med, le_postal_code], f)\n    with open('scaler.pkl', 'wb') as f:\n        pickle.dump(scaler, f)\n\n    return df_preprocessed\n\n# Example usage for each dataset size\nsizes = [500, 1000, 5000, 10000, 20000]\nnum_copies = 5  # Number of datasets to generate for each size\nbase_path = '/kaggle/working/'  # Path where the datasets are stored\n\nfor size_index, size in enumerate(sizes):\n    for copy in range(num_copies):\n        # Use 'different_causal' for all filenames to indicate unique causal relationships for each dataset size\n        relationship_label = 'different_causal'\n        filename = f'generated_data_{size}_{copy}'\n        input_path = f'{base_path}{filename}_different_causal.csv'\n        output_path = f'{base_path}preprocessed_{filename}_different_causal.csv'\n        df_preprocessed = preprocess_and_save_data(input_path, output_path)\n        print(f'Preprocessed dataset with {size} records, copy {copy + 1}, saved as {output_path}')\n        if size == 1000 and copy == 0:\n            print(df_preprocessed.head())  # Optionally display the head of the DataFrame for the first copy of 1000 records\n","metadata":{"execution":{"iopub.status.busy":"2024-08-02T23:29:42.654521Z","iopub.execute_input":"2024-08-02T23:29:42.654899Z","iopub.status.idle":"2024-08-02T23:29:48.201770Z","shell.execute_reply.started":"2024-08-02T23:29:42.654868Z","shell.execute_reply":"2024-08-02T23:29:48.200819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Model, save_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import Input, Dense, Concatenate, Embedding, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ndef create_complex_model(feature_columns, postal_code_vocab_size=30, l2_reg=0.04):\n    inputs = {name: Input(shape=(1,), name=name) for name in feature_columns}\n    embeddings = []\n    for name in feature_columns:\n        if name == 'Postal Code':\n            embedding = Embedding(input_dim=postal_code_vocab_size, output_dim=20)(inputs[name])\n            embedding = Flatten()(embedding)\n            embeddings.append(embedding)\n        else:\n            embeddings.append(inputs[name])\n\n    concatenated_inputs = Concatenate()(embeddings)\n    \n    # Add Dense layers with increased complexity\n    x = Dense(1024, activation='relu', kernel_regularizer=l2(l2_reg))(concatenated_inputs)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(512, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(256, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    # Additional Dense Layers\n    x = Dense(32, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(16, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(8, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    output = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=list(inputs.values()), outputs=output)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-08-03T00:47:46.748652Z","iopub.execute_input":"2024-08-03T00:47:46.749325Z","iopub.status.idle":"2024-08-03T00:47:46.785484Z","shell.execute_reply.started":"2024-08-03T00:47:46.749294Z","shell.execute_reply":"2024-08-03T00:47:46.784719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\n\ndef train_local_models(paths, model_creator, feature_columns, postal_code_vocab_size, target_column='Switched Medication', epochs_map=None, base_batch_size=16, learning_rate=0.0001):\n    models_path = '/kaggle/working/models/local/'\n    if not os.path.exists(models_path):\n        os.makedirs(models_path)\n    scaler = StandardScaler()\n\n    local_accuracies = {size: [] for size in paths.keys()}\n\n    for size, path_list in paths.items():\n        # Initialize the model for the entire dataset size\n        model = model_creator(feature_columns, postal_code_vocab_size)\n        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n        \n        for i, path in enumerate(path_list):\n            df = pd.read_csv(path)\n            df = df.dropna(subset=feature_columns + [target_column])\n            train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n            train_features = scaler.fit_transform(train_df[feature_columns])\n            train_features_dict = {name: train_features[:, j].reshape(-1, 1) for j, name in enumerate(feature_columns)}\n            train_labels = train_df[target_column].apply(lambda x: 1 if x == 'Yes' else 0).values\n\n            early_stopping = EarlyStopping(monitor='val_loss', patience=60, restore_best_weights=True)\n            model.fit(train_features_dict, train_labels, epochs=epochs_map[size], batch_size=base_batch_size, validation_split=0.1, callbacks=[early_stopping])\n\n            test_features = scaler.transform(test_df[feature_columns])\n            test_features_dict = {name: test_features[:, j].reshape(-1, 1) for j, name in enumerate(feature_columns)}\n            test_labels = test_df[target_column].apply(lambda x: 1 if x == 'Yes' else 0).values\n            loss, accuracy = model.evaluate(test_features_dict, test_labels, verbose=0)\n            local_accuracies[size].append(accuracy)\n\n            print(f\"Sub-dataset {i+1} - Local Model Accuracy for size {size}: {accuracy}\")\n\n        # Save the model once after all sub-datasets for this dataset size\n        model_save_path = os.path.join(models_path, f'local_model_{size}.h5')\n        model.save(model_save_path)\n        print(f\"Local Model for size {size} saved at {model_save_path}\")\n\n    return local_accuracies\n\n# Training local models\nlocal_paths = {size: [f'/kaggle/input/inputdata18/preprocessed_generated_data_{size}_{copy}_different_causal.csv' for copy in range(5)] for size in [500, 1000, 5000, 10000]}\n\nepochs_map = {500: 55, 1000: 65, 5000: 75, 10000: 85}\n\nlocal_accuracies = train_local_models(local_paths, create_complex_model, ['Age', 'Sex', 'ADHD Medication', 'Postal Code', 'Time on Medication'], postal_code_vocab_size=30, epochs_map=epochs_map)\n\n# Print the accuracies for each local model\nfor size, accuracies in local_accuracies.items():\n    print(f\"Local Model Accuracies for size {size}: {accuracies}\")\n    print(f\"Local Model Mean Accuracy for size {size}: {np.mean(accuracies)}\")\n    print(f\"Local Model Std Deviation for size {size}: {np.std(accuracies)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T00:18:22.142223Z","iopub.execute_input":"2024-08-03T00:18:22.142870Z","iopub.status.idle":"2024-08-03T00:43:53.185371Z","shell.execute_reply.started":"2024-08-03T00:18:22.142830Z","shell.execute_reply":"2024-08-03T00:43:53.184263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, Flatten, concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ndef train_federated_models(paths, model_creator, feature_columns, postal_code_vocab_size, target_column='Switched Medication', epochs_map=None, base_batch_size=16, learning_rate=0.0001):\n    models_path = '/kaggle/working/models/federated/'\n    if not os.path.exists(models_path):\n        os.makedirs(models_path)\n    scaler = StandardScaler()\n\n    federated_accuracies = {}\n\n    model = model_creator(feature_columns, postal_code_vocab_size)  # Initialize model once\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n\n    for size, path_list in paths.items():\n        size_accuracies = []\n        for i, path in enumerate(path_list):\n            df = pd.read_csv(path)\n            df = df.dropna(subset=feature_columns + [target_column])\n            features = scaler.fit_transform(df[feature_columns])\n            features_dict = {name: features[:, j].reshape(-1, 1) for j, name in enumerate(feature_columns)}\n            labels = df[target_column].apply(lambda x: 1 if x == 'Yes' else 0).values\n\n            early_stopping = EarlyStopping(monitor='val_loss', patience=60, restore_best_weights=True)\n            model.fit(features_dict, labels, epochs=epochs_map[size], batch_size=base_batch_size, validation_split=0.1, callbacks=[early_stopping])\n\n            model_save_path = os.path.join(models_path, f'federated_model_{size}_{i}.h5')\n            model.save(model_save_path)\n            print(f'Model trained on dataset {size}_{i} saved as {model_save_path}')\n\n            # Evaluate and store accuracy for this model\n            loss, accuracy = model.evaluate(features_dict, labels, verbose=0)\n            size_accuracies.append({'loss': loss, 'accuracy': accuracy})\n        \n        federated_accuracies[size] = size_accuracies\n    \n    return federated_accuracies\n\n# Paths setup for training data\nbase_path = '/kaggle/input/inputdata18/'\nsizes = [500, 1000, 5000, 10000]\nfederated_paths = {size: [f'{base_path}preprocessed_generated_data_{size}_{copy}_different_causal.csv' for copy in range(5)] for size in sizes}\n\n# Define epochs for each size\nepochs_map = {500: 55, 1000: 65, 5000: 75, 10000: 85}\n\n# Train federated models and collect accuracies\nfederated_accuracies = train_federated_models(federated_paths, create_complex_model, ['Age', 'Sex', 'ADHD Medication', 'Postal Code', 'Time on Medication'], postal_code_vocab_size=30, epochs_map=epochs_map)\n\n# Print the accuracies for each federated model\nfor size, accuracies in federated_accuracies.items():\n    print(f\"Federated Model Accuracies for size {size}: {[acc['accuracy'] for acc in accuracies]}\")\n    print(f\"Federated Model Mean Accuracy for size {size}: {np.mean([acc['accuracy'] for acc in accuracies])}\")\n    print(f\"Federated Model Std Deviation for size {size}: {np.std([acc['accuracy'] for acc in accuracies])}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T00:48:11.442129Z","iopub.execute_input":"2024-08-03T00:48:11.442469Z","iopub.status.idle":"2024-08-03T01:16:28.446325Z","shell.execute_reply.started":"2024-08-03T00:48:11.442443Z","shell.execute_reply":"2024-08-03T01:16:28.445187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport os\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef evaluate_models(local_models_path, federated_models_path, feature_columns, test_data_paths):\n    results = {'local': {}, 'federated': {}}\n    scaler = StandardScaler()\n\n    def evaluate(model, test_features, test_labels):\n        predictions = model.predict(test_features)\n        predicted_labels = (predictions > 0.5).astype(int)\n        accuracy = np.mean(predicted_labels == test_labels)\n        precision = precision_score(test_labels, predicted_labels)\n        recall = recall_score(test_labels, predicted_labels)\n        f1 = f1_score(test_labels, predicted_labels)\n        return accuracy, precision, recall, f1\n\n    # Evaluate federated models\n    federated_results = {}\n    for size, path in test_data_paths.items():\n        federated_size_results = []\n        for i in range(5):\n            model_path = os.path.join(federated_models_path, f'federated_model_{size}_{i}.h5')\n            federated_model = load_model(model_path)\n\n            df_test = pd.read_csv(path)\n            test_features_raw = scaler.fit_transform(df_test[feature_columns])\n            test_features = {name: test_features_raw[:, j].reshape(-1, 1) for j, name in enumerate(feature_columns)}\n            test_labels = df_test['Switched Medication'].apply(lambda x: 1 if x == 'Yes' else 0).values\n\n            accuracy, precision, recall, f1 = evaluate(federated_model, test_features, test_labels)\n            federated_size_results.append({'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1})\n        \n        federated_results[size] = federated_size_results\n    \n    results['federated'] = federated_results\n\n    # Evaluate local models\n    local_results = {}\n    for size, path in test_data_paths.items():\n        local_size_results = []\n        for i in range(5):\n            model_path = os.path.join(local_models_path, f'local_model_{size}.h5')\n            local_model = load_model(model_path)\n\n            df_test = pd.read_csv(path)\n            test_features_raw = scaler.fit_transform(df_test[feature_columns])\n            test_features = {name: test_features_raw[:, j].reshape(-1, 1) for j, name in enumerate(feature_columns)}\n            test_labels = df_test['Switched Medication'].apply(lambda x: 1 if x == 'Yes' else 0).values\n\n            accuracy, precision, recall, f1 = evaluate(local_model, test_features, test_labels)\n            local_size_results.append({'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1})\n        \n        local_results[size] = local_size_results\n    \n    results['local'] = local_results\n\n    return results\n\n# Paths for the test datasets ending with _0\ntest_data_paths = {size: f'/kaggle/input/inputdata18/preprocessed_generated_data_{size}_0_different_causal.csv' for size in [500, 1000, 5000, 10000]}\n\nlocal_models_path = '/kaggle/working/models/local/'\nfederated_models_path = '/kaggle/working/models/federated/'\n\nevaluation_results = evaluate_models(local_models_path, federated_models_path, ['Age', 'Sex', 'ADHD Medication', 'Postal Code', 'Time on Medication'], test_data_paths)\n\n# Print the evaluation results\nfor model_type, model_results in evaluation_results.items():\n    print(f\"Results for {model_type} models:\")\n    for size, metrics_list in model_results.items():\n        print(f\"Dataset size: {size}\")\n        for i, metrics in enumerate(metrics_list):\n            print(f\"  Sub-dataset {i}:\")\n            for metric, value in metrics.items():\n                print(f\"    {metric}: {value:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T01:17:53.925591Z","iopub.execute_input":"2024-08-03T01:17:53.926222Z","iopub.status.idle":"2024-08-03T01:19:01.469561Z","shell.execute_reply.started":"2024-08-03T01:17:53.926188Z","shell.execute_reply":"2024-08-03T01:19:01.468575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_final_accuracy_with_repeats_extended(local_accuracies, federated_metrics, sizes, metric, save_path):\n    plt.figure(figsize=(12, 8))\n\n    # Plot individual metrics for local models\n    for size in sizes:\n        repeats = [m[metric] for m in local_accuracies[size]]\n        plt.scatter([size]*len(repeats), repeats, color='blue', alpha=0.5, label=f'Local {metric.capitalize()}' if size == sizes[0] else \"\")\n\n    # Plot individual metrics for federated models\n    for size in sizes:\n        federated_repeats = [m[metric] for m in federated_metrics[size]]\n        plt.scatter([size]*len(federated_repeats), federated_repeats, color='orange', alpha=0.5, label=f'Federated {metric.capitalize()}' if size == sizes[0] else \"\")\n\n    # Calculate means and std deviations for line chart with error bars\n    local_means = [np.mean([m[metric] for m in local_accuracies[size]]) for size in sizes]\n    local_stds = [np.std([m[metric] for m in local_accuracies[size]]) for size in sizes]\n    federated_means = [np.mean([m[metric] for m in federated_metrics[size]]) for size in sizes]\n    federated_stds = [np.std([m[metric] for m in federated_metrics[size]]) for size in sizes]\n\n    # Plot error bars\n    plt.errorbar(sizes, local_means, yerr=local_stds, fmt='-o', color='blue', label='Local Mean')\n    plt.errorbar(sizes, federated_means, yerr=federated_stds, fmt='--o', color='orange', label='Federated Mean')\n\n    plt.xlabel('Dataset Size')\n    plt.ylabel(metric.capitalize())\n    plt.title(f'{metric.capitalize()} vs. Dataset Size (Local and Federated Models)')\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(f'{save_path}/{metric}_comparison_model.png')\n    plt.show()\n\n# Plotting with individual accuracies and error bars\nplot_final_accuracy_with_repeats_extended(evaluation_results['local'], evaluation_results['federated'], [500, 1000, 5000, 10000], 'accuracy', '/kaggle/working/')\nplot_final_accuracy_with_repeats_extended(evaluation_results['local'], evaluation_results['federated'], [500, 1000, 5000, 10000], 'precision', '/kaggle/working/')\nplot_final_accuracy_with_repeats_extended(evaluation_results['local'], evaluation_results['federated'], [500, 1000, 5000, 10000], 'recall', '/kaggle/working/')\nplot_final_accuracy_with_repeats_extended(evaluation_results['local'], evaluation_results['federated'], [500, 1000, 5000, 10000], 'f1', '/kaggle/working/')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T01:19:19.555492Z","iopub.execute_input":"2024-08-03T01:19:19.556322Z","iopub.status.idle":"2024-08-03T01:19:22.477560Z","shell.execute_reply.started":"2024-08-03T01:19:19.556278Z","shell.execute_reply":"2024-08-03T01:19:22.476638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Appendix Stage 3","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, concatenate, Dense, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.models import load_model, Model\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport uuid\nimport tensorflow as tf\n\n# Function to set layers of a model as non-trainable\ndef set_non_trainable(model):\n    for layer in model.layers:\n        layer.trainable = False\n    return model\n\n# Function to clear existing datasets in the HDF5 file\ndef clear_hdf5_datasets(file_path):\n    try:\n        with h5py.File(file_path, 'a') as f:\n            for key in list(f.keys()):\n                del f[key]\n            print(f\"Cleared existing datasets in {file_path}\")\n    except Exception as e:\n        print(f\"Error while clearing HDF5 file: {e}\")\n\nimport h5py\nimport time\n\ndef train_combined_model(combined_model, X_train_local, X_train_federated, y_train, X_test_local, X_test_federated, y_test, model_name, unique_id, epochs=50, batch_size=32):\n    # Prepare the input data for the combined model\n    X_train_combined = [np.array(data) for data in X_train_local + X_train_federated]\n    X_test_combined = [np.array(data) for data in X_test_local + X_test_federated]\n    \n    # Ensure labels are numpy arrays\n    y_train = np.array(y_train)\n    y_test = np.array(y_test)\n\n    combined_model.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryCrossentropy(), metrics=['accuracy'])\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n    \n    history = combined_model.fit(X_train_combined, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping, lr_scheduler])\n    evaluation = combined_model.evaluate(X_test_combined, y_test)\n    \n    # Save the combined model\n    save_path = f\"combined_{model_name}_{unique_id}.h5\"\n    \n    # Ensure the file does not exist before saving\n    if os.path.exists(save_path):\n        os.remove(save_path)\n    \n    # Try saving the model and catch any potential errors\n    try:\n        combined_model.save(save_path)\n        print(f\"Model saved successfully to: {save_path}\")\n    except Exception as e:\n        print(f\"Error while saving model: {e}\")\n        \n        # Debugging: Check contents of the HDF5 file if saving fails\n        if os.path.exists(save_path):\n            with h5py.File(save_path, 'r') as f:\n                print(\"Contents of the HDF5 file before saving:\")\n                f.visit(print)\n                f.close()  # Explicitly close the file\n            \n            # Wait for a moment to ensure all operations are completed\n            time.sleep(1)\n            \n            # Clear existing datasets if any\n            with h5py.File(save_path, 'a') as f:\n                if 'model_weights' in f:\n                    for key in list(f['model_weights'].keys()):\n                        del f['model_weights'][key]\n                        print(f\"Deleted dataset: {key}\")\n                    time.sleep(1)  # Ensure deletion is completed\n                    f.visit(print)\n                f.close()  # Explicitly close the file\n            \n            # Retry saving the model\n            try:\n                combined_model.save(save_path)\n                print(f\"Model saved successfully to: {save_path}\")\n            except Exception as e:\n                print(f\"Retry failed: {e}\")\n    \n    return history, evaluation\n\n# Function to load data for stage 1 (dummy function for demonstration)\ndef load_data_for_stage(stage, size, data_paths):\n    # Dummy data for demonstration purposes\n    # Replace this with actual data loading logic\n    X_train = [np.random.rand(100, 1) for _ in range(5)]\n    y_train = np.random.randint(2, size=100)\n    X_test = [np.random.rand(20, 1) for _ in range(5)]\n    y_test = np.random.randint(2, size=20)\n    return X_train, y_train, X_test, y_test\n\n# Function to load data for federated model (dummy function for demonstration)\ndef load_data_for_federated(size, data_paths):\n    # Dummy data for demonstration purposes\n    # Replace this with actual data loading logic\n    X_train = [np.random.rand(100, 1) for _ in range(5)]\n    y_train = np.random.randint(2, size=100)\n    X_test = [np.random.rand(20, 1) for _ in range(5)]\n    y_test = np.random.randint(2, size=20)\n    return X_train, y_train, X_test, y_test\n\n# Function to check if the input shape is valid\ndef validate_input_shape(shape):\n    if isinstance(shape, tuple) and all(isinstance(dim, int) for dim in shape):\n        return True\n    return False\n\n# Function to create a locally enhanced model\ndef create_locally_enhanced_model(local_model, federated_model, unique_id):\n    # Clear the session to reset the layer naming\n    K.clear_session()\n\n    # Set federated model layers as non-trainable\n    federated_model = set_non_trainable(federated_model)\n\n    # Print input shapes for debugging\n    print(f\"Local model input shape: {local_model.input_shape}\")\n    print(f\"Federated model input shape: {federated_model.input_shape}\")\n\n    # Create input layers for local model\n    local_inputs = [Input(shape=shape[1:], name=f'input_local_{i}_{unique_id}') for i, shape in enumerate(local_model.input_shape)]\n    local_outputs = local_model(local_inputs)\n    if isinstance(local_outputs, list):\n        local_outputs = concatenate(local_outputs, name=f'concat_local_{unique_id}')\n\n    # Create input layers for federated model\n    federated_inputs = [Input(shape=shape[1:], name=f'input_federated_{i}_{unique_id}') for i, shape in enumerate(federated_model.input_shape)]\n    federated_outputs = federated_model(federated_inputs)\n    if isinstance(federated_outputs, list):\n        federated_outputs = concatenate(federated_outputs, name=f'concat_federated_{unique_id}')\n\n    # Concatenate local and federated outputs\n    combined_output = concatenate([local_outputs, federated_outputs], name=f'concat_combined_{unique_id}')\n    combined_output = BatchNormalization(name=f'batch_norm_{unique_id}')(combined_output)\n    combined_output = Dropout(0.5, name=f'dropout_combined_{unique_id}')(combined_output)\n    combined_output = Dense(128, activation='relu', name=f'dense_1_{unique_id}')(combined_output)\n    combined_output = Dropout(0.5, name=f'dropout_1_{unique_id}')(combined_output)\n    combined_output = Dense(64, activation='relu', name=f'dense_2_{unique_id}')(combined_output)\n    combined_output = Dropout(0.5, name=f'dropout_2_{unique_id}')(combined_output)\n    final_output = Dense(1, activation='sigmoid', name=f'dense_output_{unique_id}')(combined_output)\n\n    # Return the combined model\n    combined_model = Model(inputs=local_inputs + federated_inputs, outputs=final_output, name=f'combined_model_{unique_id}')\n    return combined_model\n\n# Function to train and evaluate combined models\ndef create_train_evaluate_combined_models(model_paths_stage_1, federated_model_path, size, stage_1_data_paths, federated_data_paths, unique_id):\n    X_train_stage_1, y_train_stage_1, X_test_stage_1, y_test_stage_1 = load_data_for_stage(1, size, stage_1_data_paths)\n    X_train_federated, y_train_federated, X_test_federated, y_test_federated = load_data_for_federated(size, federated_data_paths)\n\n    combined_results = {}\n    \n    for model_name_stage_1, model_path_stage_1 in model_paths_stage_1.items():\n        if model_name_stage_1.split('_')[-1] == str(size):  # Match the size\n            print(f\"Creating combined model for {model_name_stage_1} and federated model\")\n            local_model = load_model(model_path_stage_1)\n            federated_model = load_model(federated_model_path)\n\n            combined_model = create_locally_enhanced_model(local_model, federated_model, unique_id)\n            combined_model_name = f\"{model_name_stage_1}_and_federated_{unique_id}\"\n            print(f\"Training combined model for {model_name_stage_1} and federated model\")\n            history, evaluation = train_combined_model(combined_model, X_train_stage_1, X_train_federated, y_train_stage_1, X_test_stage_1, X_test_federated, y_test_stage_1, combined_model_name, unique_id, epochs=50, batch_size=32)\n            combined_results[combined_model_name] = evaluation\n            print(f\"Evaluation for {combined_model_name}: {evaluation}\")\n            plot_history(history, f\"Combined Model {combined_model_name}\")\n    \n    return combined_results\n\n# Function to plot training history\ndef plot_history(history, title):\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n    plt.title(f'{title} - Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Val Loss')\n    plt.title(f'{title} - Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n# Define model paths for Stage 1 Local Models\nstage_1_local_model_paths = {\n    'local_model_1000': '/kaggle/input/stage_1_local/tensorflow2/default/1/local_model_1000.h5',\n    'local_model_500': '/kaggle/input/stage_1_local/tensorflow2/default/1/local_model_500.h5',\n    'local_model_10000': '/kaggle/input/stage_1_local/tensorflow2/default/1/local_model_10000.h5',\n    'local_model_5000': '/kaggle/input/stage_1_local/tensorflow2/default/1/local_model_5000.h5'\n}\n\n# Define model paths for Stage 2 Local Models\nstage_2_local_model_paths = {\n    'local_model_1000': '/kaggle/input/stage_2_local/tensorflow2/default/1/local_model_1000.h5',\n    'local_model_500': '/kaggle/input/stage_2_local/tensorflow2/default/1/local_model_500.h5',\n    'local_model_10000': '/kaggle/input/stage_2_local/tensorflow2/default/1/local_model_10000.h5',\n    'local_model_5000': '/kaggle/input/stage_2_local/tensorflow2/default/1/local_model_5000.h5'\n}\n\n# Define model paths for Stage 1 Federated Models\nstage_1_federated_model_paths = {\n    'federated_model_1000_0': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_1000_0.h5',\n    'federated_model_1000_1': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_1000_1.h5',\n    'federated_model_1000_2': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_1000_2.h5',\n    'federated_model_1000_3': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_1000_3.h5',\n    'federated_model_1000_4': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_1000_4.h5',\n    'federated_model_10000_0': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_10000_0.h5',\n    'federated_model_10000_1': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_10000_1.h5',\n    'federated_model_10000_2': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_10000_2.h5',\n    'federated_model_10000_3': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_10000_3.h5',\n    'federated_model_10000_4': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_10000_4.h5',\n    'federated_model_500_0': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_500_0.h5',\n    'federated_model_500_1': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_500_1.h5',\n    'federated_model_500_2': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_500_2.h5',\n    'federated_model_500_3': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_500_3.h5',\n    'federated_model_500_4': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_500_4.h5',\n    'federated_model_5000_0': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_5000_0.h5',\n    'federated_model_5000_1': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_5000_1.h5',\n    'federated_model_5000_2': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_5000_2.h5',\n    'federated_model_5000_3': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_5000_3.h5',\n    'federated_model_5000_4': '/kaggle/input/stage_1_federated/tensorflow2/default/1/federated_model_5000_4.h5'\n}\n\n# Define model paths for Stage 2 Federated Models\nstage_2_federated_model_paths = {\n    'federated_model_1000_0': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_1000_0.h5',\n    'federated_model_1000_1': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_1000_1.h5',\n    'federated_model_1000_2': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_1000_2.h5',\n    'federated_model_1000_3': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_1000_3.h5',\n    'federated_model_1000_4': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_1000_4.h5',\n    'federated_model_10000_0': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_10000_0.h5',\n    'federated_model_10000_1': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_10000_1.h5',\n    'federated_model_10000_2': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_10000_2.h5',\n    'federated_model_10000_3': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_10000_3.h5',\n    'federated_model_10000_4': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_10000_4.h5',\n    'federated_model_500_0': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_500_0.h5',\n    'federated_model_500_1': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_500_1.h5',\n    'federated_model_500_2': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_500_2.h5',\n    'federated_model_500_3': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_500_3.h5',\n    'federated_model_500_4': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_500_4.h5',\n    'federated_model_5000_0': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_5000_0.h5',\n    'federated_model_5000_1': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_5000_1.h5',\n    'federated_model_5000_2': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_5000_2.h5',\n    'federated_model_5000_3': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_5000_3.h5',\n    'federated_model_5000_4': '/kaggle/input/stage_2_federated/tensorflow2/default/1/federated_model_5000_4.h5'\n}\n\nsizes = [1000, 500, 10000, 5000]  # Example sizes, replace with actual sizes\n\n# Stage 1 data paths\nstage_1_data_paths = {size: [f'/kaggle/input/inputdata18/preprocessed_data_{size}_{copy}.csv' for copy in range(5)] for size in sizes}\n\n# Stage 2 data paths\nstage_2_data_paths = {size: [f'/kaggle/input/inputdata18/preprocessed_generated_data_{size}_{copy}_different_causal.csv' for copy in range(5)] for size in sizes}\n\nresults_combined_local_stage_1 = {size: {} for size in sizes}\nresults_combined_local_stage_2 = {size: {} for size in sizes}\n\nprint(\"Combining Stage 1 Local Models with Federated Models\")\nfor size in sizes:\n    federated_model_path = stage_1_federated_model_paths[f'federated_model_{size}_0']  # Selecting one of the federated models\n    results_combined_local_stage_1[size] = create_train_evaluate_combined_models(stage_1_local_model_paths, federated_model_path, size, stage_1_data_paths, stage_1_data_paths, f\"local_{size}\")\n\nprint(\"Combining Stage 2 Local Models with Federated Models\")\nfor size in sizes:\n    federated_model_path = stage_2_federated_model_paths[f'federated_model_{size}_0']  # Selecting one of the federated models\n    results_combined_local_stage_2[size] = create_train_evaluate_combined_models(stage_2_local_model_paths, federated_model_path, size, stage_2_data_paths, stage_2_data_paths, f\"local_{size}\")\n\nimport pandas as pd\n\ndef summarize_results(results, model_type):\n    summary = []\n    for size, models in results.items():\n        for model_name, evaluation in models.items():\n            summary.append({\n                'Model': model_name,\n                'Size': size,\n                'Loss': evaluation[0],\n                'Accuracy': evaluation[1],\n                'Model Type': model_type\n            })\n    return pd.DataFrame(summary)\n\ndf_combined_local_stage_1 = summarize_results(results_combined_local_stage_1, 'Local Stage 1')\ndf_combined_local_stage_2 = summarize_results(results_combined_local_stage_2, 'Local Stage 2')\n\n# Combine both DataFrames\ndf_combined_local = pd.concat([df_combined_local_stage_1, df_combined_local_stage_2])\n\n# Display results\nprint(df_combined_local)\n\n# Plotting results\nplt.figure(figsize=(14, 8))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nfor model_type in ['Local Stage 1', 'Local Stage 2']:\n    df_subset = df_combined_local[df_combined_local['Model Type'] == model_type]\n    plt.plot(df_subset['Size'], df_subset['Accuracy'], label=f'{model_type}')\nplt.title('Model Accuracy Comparison')\nplt.xlabel('Dataset Size')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Loss plot\nplt.subplot(1, 2, 2)\nfor model_type in ['Local Stage 1', 'Local Stage 2']:\n    df_subset = df_combined_local[df_combined_local['Model Type'] == model_type]\n    plt.plot(df_subset['Size'], df_subset['Loss'], label=f'{model_type}')\nplt.title('Model Loss Comparison')\nplt.xlabel('Dataset Size')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T01:50:38.105324Z","iopub.execute_input":"2024-08-03T01:50:38.105721Z","iopub.status.idle":"2024-08-03T02:04:02.055001Z","shell.execute_reply.started":"2024-08-03T01:50:38.105689Z","shell.execute_reply":"2024-08-03T02:04:02.054095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming we have similar functions to create and evaluate federated models\nresults_combined_federated_stage_1 = {size: {} for size in sizes}\nresults_combined_federated_stage_2 = {size: {} for size in sizes}\n\nprint(\"Combining Stage 1 Federated Models\")\nfor size in sizes:\n    federated_model_path = stage_1_federated_model_paths[f'federated_model_{size}_0']  # Selecting one of the federated models\n    results_combined_federated_stage_1[size] = create_train_evaluate_combined_models(stage_1_federated_model_paths, federated_model_path, size, stage_1_data_paths, stage_1_data_paths, f\"federated_{size}\")\n\nprint(\"Combining Stage 2 Federated Models\")\nfor size in sizes:\n    federated_model_path = stage_2_federated_model_paths[f'federated_model_{size}_0']  # Selecting one of the federated models\n    results_combined_federated_stage_2[size] = create_train_evaluate_combined_models(stage_2_federated_model_paths, federated_model_path, size, stage_2_data_paths, stage_2_data_paths, f\"federated_{size}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T02:06:00.966477Z","iopub.execute_input":"2024-08-03T02:06:00.966857Z","iopub.status.idle":"2024-08-03T02:06:00.977138Z","shell.execute_reply.started":"2024-08-03T02:06:00.966829Z","shell.execute_reply":"2024-08-03T02:06:00.976150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Combine results for local models from Stage 1 and Stage 2\ndf_combined_local_stage_1 = summarize_results(results_combined_local_stage_1, 'Local Stage 1')\ndf_combined_local_stage_2 = summarize_results(results_combined_local_stage_2, 'Local Stage 2')\ndf_combined_local = pd.concat([df_combined_local_stage_1, df_combined_local_stage_2])\n\n# Combine results for federated models from Stage 1 and Stage 2\ndf_combined_federated_stage_1 = summarize_results(results_combined_federated_stage_1, 'Federated Stage 1')\ndf_combined_federated_stage_2 = summarize_results(results_combined_federated_stage_2, 'Federated Stage 2')\ndf_combined_federated = pd.concat([df_combined_federated_stage_1, df_combined_federated_stage_2])\n\n# Combine all DataFrames\ndf_combined_all = pd.concat([df_combined_local, df_combined_federated])\n\n# Ensure data is sorted by Size and Model Type for correct plotting\ndf_combined_all = df_combined_all.sort_values(by=['Size', 'Model Type'])\n\n# Plotting results\nplt.figure(figsize=(14, 8))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nfor model_type in df_combined_all['Model Type'].unique():\n    df_subset = df_combined_all[df_combined_all['Model Type'] == model_type]\n    plt.plot(df_subset['Size'], df_subset['Accuracy'], marker='o', label=f'{model_type}')\n\nplt.title('Model Accuracy Comparison')\nplt.xlabel('Dataset Size')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\n\n# Loss plot\nplt.subplot(1, 2, 2)\nfor model_type in df_combined_all['Model Type'].unique():\n    df_subset = df_combined_all[df_combined_all['Model Type'] == model_type]\n    plt.plot(df_subset['Size'], df_subset['Loss'], marker='o', label=f'{model_type}')\n\nplt.title('Model Loss Comparison')\nplt.xlabel('Dataset Size')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T02:06:09.219802Z","iopub.execute_input":"2024-08-03T02:06:09.220520Z","iopub.status.idle":"2024-08-03T02:06:10.032396Z","shell.execute_reply.started":"2024-08-03T02:06:09.220487Z","shell.execute_reply":"2024-08-03T02:06:10.031519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}